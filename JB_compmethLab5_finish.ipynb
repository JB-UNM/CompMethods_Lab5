{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "direct-asian",
   "metadata": {},
   "source": [
    "# Lab 5. Abstraction and reusability\n",
    "#### Computational Methods for Geoscience - EPS 400/522\n",
    "#### Instructor: Eric Lindsey\n",
    "\n",
    "Due: Oct. 5, 2023\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "center-trail",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# some useful imports and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "# better looking figures on high-resolution screens\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# reload modules if they have changed - necessary when you are editing your own module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0cc41c-af4c-4539-9352-260e9eb3dee3",
   "metadata": {},
   "source": [
    "### 1. Using glob to find files\n",
    "\n",
    "The folder 'timeseries' (you will have to unzip it first) contains a set of GNSS timeseries from the UNR MAGNET site. Let's explore how 'glob' can interact with these files.\n",
    "\n",
    "1. Use glob to get a list of all the files, and print out each filename.\n",
    "\n",
    "2. The sites starting with a letter 'P' were installed under a single project called the 'Plate Boundary Observatory'. Suppose we wanted to list only those files - can you use 'glob' with wildcards to return only the list of names starting with P?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e030db2e-4898-4a86-a986-1f2ab70f0c27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name and pathway of file found:  ./timeseries\\AZCN.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\CTI4.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\MC10.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\NMLG.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\P028.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\P029.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\P034.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\RG01.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\SC01.NA.tenv3\n",
      "Name and pathway of file found:  ./timeseries\\TC01.NA.tenv3\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os.path\n",
    "\n",
    "# Adapted to this to be relative to your home directory. ~ expands out to C:\\\\Users\\your_username, so if you put\n",
    "# these in the same location we can just use the same code without having to adjust the file path when\n",
    "# switching between our files\n",
    "\n",
    "file_location = './timeseries/'\n",
    "file_list = glob.glob(file_location + '*tenv3')\n",
    "for file in file_list:\n",
    "    print('Name and pathway of file found: ', file)\n",
    "    \n",
    "PBO_files = glob.glob('/Users/jasonboryszewski/Downloads/timeseries/P*.tenv3')\n",
    "for file in PBO_files:\n",
    "    print('These are the files from the Plate Boundary Observator project: ', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fefebe-70f9-443d-abc5-7766de048ee0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Write a module to interact with the GNSS timeseries\n",
    "\n",
    "The module should have (at a minimum) the following four functions with their definitions:\n",
    "\n",
    "fit_timeseries(tlist,ylist) - accepts two lists: t (decimal year) and y (displacement timeseries)  as 1-D numpy arrays, and returns the least-squares velocity and uncertainty for that timeseries. If possible, try to re-use the line-fitting code you wrote for Lab 3 for this purpose.\n",
    "\n",
    "fit_velocities(filename) - accepts a filename, reads in the data, and uses fit_timeseries() to estimate the E, N and U components of velocity for that site.\n",
    "\n",
    "get_coordinates(filename) - accepts a filename and returns the average latitude, longitude, and elevation for that site over the time period.\n",
    "\n",
    "fit_all_velocities(folder,pattern) - accepts a folder name and a 'glob' pattern and returns a pandas data frame with the site name, coordinates, velocities and uncertainties.\n",
    "\n",
    "Finally, import your module and demonstrate each function below to show how it works and what it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca82262-d162-4034-b43f-5b84f56925d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#I have this saved as a module in my file, but wanted to put it here as well so it's\n",
    "#visible in the submitted html\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "def fit_timeseries(tlist,ylist):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(tlist, ylist)\n",
    "    velocity = round(slope,7)\n",
    "    uncertainty = round(std_err,7)\n",
    "    #print(f'This is the velocity and uncertainty for this timeseries: {velocity}, {uncertainty}')\n",
    "    return velocity, uncertainty\n",
    "\n",
    "def fit_velocities(filename):\n",
    "    dset = pd.read_csv(filename, delim_whitespace=True)\n",
    "    \n",
    "    change_east = dset['__east(m)']\n",
    "    change_north = dset['_north(m)']\n",
    "    change_up = dset['____up(m)']\n",
    "    time = dset['yyyy.yyyy']\n",
    "    \n",
    "    e_velocity, e_uncertainty = fit_timeseries(time, change_east)\n",
    "    n_velocity, n_uncertainty = fit_timeseries(time, change_north)\n",
    "    u_velocity, u_uncertainty = fit_timeseries(time, change_up)\n",
    "    return e_velocity, e_uncertainty, n_velocity, n_uncertainty, u_velocity, u_uncertainty\n",
    "\n",
    "def get_coordinates(filename):\n",
    "    dset = pd.read_csv(filename, delim_whitespace=True)\n",
    "    \n",
    "    latlist = dset['_latitude(deg)']\n",
    "    lonlist = dset['_longitude(deg)']\n",
    "    heightlist = dset['__height(m)']\n",
    "    \n",
    "    avg_lat = np.mean(latlist)\n",
    "    avg_lon = np.mean(lonlist)\n",
    "    avg_height = np.mean(heightlist)\n",
    "    #print(f'Avg lat: {avg_lat}; Avg lon: {avg_lon}; Avg height: {avg_height}')\n",
    "    \n",
    "    return avg_lat, avg_lon, avg_height\n",
    "\n",
    "def fit_all_velocities(folder,pattern):\n",
    "    results = []\n",
    "    file_list = glob.glob('/Users/jasonboryszewski/Downloads/timeseries/*.tenv3')\n",
    "    \n",
    "    for file in file_list:\n",
    "        filename = os.path.basename(file)\n",
    "        sitename = filename.split('.')[0]\n",
    "        coordinates = get_coordinates(file)\n",
    "        velocities = fit_velocities(file)\n",
    "        dset = pd.read_csv(filename, delim_whitespace=True)\n",
    "        \n",
    "        site_info = {\n",
    "            'sitename': sitename,\n",
    "            'avg_lat': coordinates[0],\n",
    "            'avg_lon': coordinates[1],\n",
    "            'avg_height': coordinates[2],\n",
    "            'e_velocity': velocities[0],\n",
    "            'e_uncertainty': velocities[1],\n",
    "            'n_velocity': velocities[2],\n",
    "            'n_uncertainty': velocities[3],\n",
    "            'u_velocity': velocities[4],\n",
    "            'u_uncertainty': velocities[5]\n",
    "            }\n",
    "        results.append(site_info)\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd8b8310-4c42-481d-8a2e-4b3d33b88930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AZNC.NA.tenv3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m ts\u001b[38;5;241m.\u001b[39mfit_timeseries(tlist,ylist)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#given a filename, the fit_velocities function will use the fit_timeseries function to fit\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#a linear regression to each of the east, north and up lists and return velocity and uncertainty\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#for each\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m ts\u001b[38;5;241m.\u001b[39mfit_velocities(filename)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#given a filename, the get_coordinates function will calculate average lat, lon and height\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#and return those averages\u001b[39;00m\n\u001b[0;32m     20\u001b[0m ts\u001b[38;5;241m.\u001b[39mget_coordinates(filename) \n",
      "File \u001b[1;32m~\\Documents\\GitHub\\CompMethods_Lab5\\timeseries_module.py:15\u001b[0m, in \u001b[0;36mfit_velocities\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m def fit_velocities(filename):\n\u001b[1;32m---> 15\u001b[0m     dset = pd.read_csv(filename, delim_whitespace=True)\n\u001b[0;32m     16\u001b[0m     \n\u001b[0;32m     17\u001b[0m     change_east = dset['__east(m)']\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AZNC.NA.tenv3'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import timeseries_module as ts\n",
    "\n",
    "filename = 'AZNC.NA.tenv3'\n",
    "folder = './timeseries'\n",
    "pattern = '*tenv3'\n",
    "\n",
    "\n",
    "#given a tlist and ylist, the fit_timeseries function will return velocity and uncertainty of\n",
    "#the linear regression\n",
    "tlist=[0 , 1000, 5]\n",
    "ylist=[0,200, 3]\n",
    "ts.fit_timeseries(tlist,ylist)\n",
    "\n",
    "#given a filename, the fit_velocities function will use the fit_timeseries function to fit\n",
    "#a linear regression to each of the east, north and up lists and return velocity and uncertainty\n",
    "#for each\n",
    "ts.fit_velocities(filename)\n",
    "\n",
    "#given a filename, the get_coordinates function will calculate average lat, lon and height\n",
    "#and return those averages\n",
    "ts.get_coordinates(filename) \n",
    "\n",
    "#the fit_all_velocities function will use a for loop to run through a list of files, given a\n",
    "#folder and pattern, and use the previous functions to calculate averages and timeseries\n",
    "#for each file and append to a pandas dataframe\n",
    "ts.fit_all_velocities(folder, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff7646-860f-4a87-accd-a77e1c41d0a4",
   "metadata": {},
   "source": [
    "### 3. Upload the module to GitHub, along with a README.md file explaining briefly how to use it.\n",
    "\n",
    "Enter a link to your GitHub repository here for me to check out: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c8fb3-05cd-486f-8cac-32ff52075e84",
   "metadata": {},
   "source": [
    "https://github.com/JB-UNM/CompMethods_Lab5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8beca1f-c264-4bb0-8bdb-393a3f1194b3",
   "metadata": {},
   "source": [
    "### 4. Use the timeseries calculation module you created\n",
    "\n",
    "Using at most 5 lines of code, import the module you created above and use it to estimate the timeseries for all 10 of the sites, print them out, and save the results to a new file 'site_velocities.csv'. Feel free to download more sites as well and put them in the folder too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1394f71a-8eba-42d7-81a9-23c5b108a9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bug tracker!\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\tlee4\\AppData\\Local\\Temp\\ipykernel_24908\\2911163010.py\", line 2, in <module>\n",
      "    data = ts.fit_all_velocities(file_location,'*.tenv3')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\Documents\\GitHub\\CompMethods_Lab5\\timeseries_module.py\", line 53, in fit_all_velocities\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 950, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 605, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1442, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1735, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py\", line 856, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'AZCN.NA.tenv3'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\tlee4\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import timeseries_module\n",
    "data = ts.fit_all_velocities(file_location,'*.tenv3')\n",
    "data.to_csv('site_velocities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-terminology",
   "metadata": {},
   "source": [
    "### 5. Re-use your module to estimate sea level rise rates\n",
    "\n",
    "Go to the following page and download at least 5 monthly sea level timeseries spanning at least 100 years: https://psmsl.org/products/gloss/glossmap.html. Place them in a new folder.\n",
    "\n",
    "(To download the data: click a station icon on the map, then click the station number/name (first link in the pop-up, e.g. \"155: Honolulu\". Then right-click the link next to the plot of monthly data (\"Download monthly mean sea level data.\") and save it as a file.)\n",
    "\n",
    "Now, create a new function \"fit_tide_gauge\" in your module that re-uses your function \"fit_timeseries\" to return the relative sea level rate of change for a given station. \n",
    "\n",
    "Next, modify your function \"fit_all_velocities\" to accept a \"type\" parameter (GNSS or tide), and re-use it to estimate the rates for all the tide gauges you downloaded. Print out the results below.\n",
    "\n",
    "Finally, update your github repository with this new version of the module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180c55ea-4d26-4dbd-84a3-8a6bc9619c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have this saved as a module, but just put it here too so it's visible in\n",
    "#submitted html\n",
    "from scipy import stats\n",
    "\n",
    "def fit_tide_gauge(time, sealevel, sealvl_file):\n",
    "    dset_sealvl = pd.read_csv(sealvl_file, header=None, sep=';')\n",
    "\n",
    "    time = dset_sealvl[0]\n",
    "    sealevel = dset_sealvl[1]\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(time, sealevel)\n",
    "    \n",
    "    return slope\n",
    "\n",
    "def all_tide_changes(folder, pattern):\n",
    "    tide_changes = []\n",
    "    rlr_list = glob.glob(os.path.join(folder, pattern))\n",
    "\n",
    "    for file in rlr_list:\n",
    "        filename = os.path.basename(file)\n",
    "        sitename = filename.split('.')[0]\n",
    "        slope = fit_tide_gauge(None, None, file)  # Pass the filename to the function\n",
    "\n",
    "        if slope is not None:\n",
    "            site_info = {'sitename': sitename, 'rate_change': slope}\n",
    "            tide_changes.append(site_info)\n",
    "\n",
    "    df = pd.DataFrame(tide_changes)\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bd0e61-8df7-430d-b27c-6fc2bd199691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a filename from the sea_level_rise folder:  1196.rlrdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sitename  rate_change\n",
      "0      150   -33.134862\n",
      "1      487   107.734500\n",
      "2     1154   108.098319\n",
      "3     1196     1.711255\n",
      "4      359   105.034583\n",
      "5      155     1.549289\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sitename</th>\n",
       "      <th>rate_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>-33.134862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>487</td>\n",
       "      <td>107.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1154</td>\n",
       "      <td>108.098319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1196</td>\n",
       "      <td>1.711255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359</td>\n",
       "      <td>105.034583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>1.549289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sitename  rate_change\n",
       "0      150   -33.134862\n",
       "1      487   107.734500\n",
       "2     1154   108.098319\n",
       "3     1196     1.711255\n",
       "4      359   105.034583\n",
       "5      155     1.549289"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeseries_module as ts\n",
    "time = None\n",
    "sealevel = None\n",
    "sealvl_file = input('Enter a filename from the sea_level_rise folder: ')\n",
    "folder = '/Users/jasonboryszewski/Downloads/*.rlrdata'\n",
    "pattern = '/Users/jasonboryszewski/Downloads/*.rlrdata'\n",
    "\n",
    "ts.fit_tide_gauge(time, sealevel, sealvl_file)\n",
    "ts.all_tide_changes(folder,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d8d3f-99f3-431c-8906-836e3a9872e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
